{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f2fbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spinal_pain.pdf...\n",
      "Detected Body Font Size: ~10pt\n",
      "Scanning Page 1...\n",
      "Scanning Page 2...\n",
      "Scanning Page 3...\n",
      "Scanning Page 4...\n",
      "Scanning Page 5...\n",
      "Scanning Page 6...\n",
      "Scanning Page 7...\n",
      "Scanning Page 8...\n",
      "Scanning Page 9...\n",
      "Scanning Page 10...\n",
      "Scanning Page 11...\n",
      "Scanning Page 12...\n",
      "Scanning Page 13...\n",
      "Scanning Page 14...\n",
      "Successfully saved to spinal_pain.md\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_font_sizes(pdf):\n",
    "    \"\"\"\n",
    "    Scans the PDF to determine the most common font size (Body Text)\n",
    "    and identify sizes likely to be headers.\n",
    "    \"\"\"\n",
    "    font_sizes = []\n",
    "    for page in pdf.pages:\n",
    "        # Get all words and their heights\n",
    "        words = page.extract_words(extra_attrs=[\"size\"])\n",
    "        for w in words:\n",
    "            if w['size']:\n",
    "                # Round to nearest integer to group similar sizes\n",
    "                font_sizes.append(round(w['size']))\n",
    "    \n",
    "    if not font_sizes:\n",
    "        return 10, {} # Fallback\n",
    "\n",
    "    # The most common font size is likely the Body Text\n",
    "    size_counts = Counter(font_sizes)\n",
    "    body_font_size = size_counts.most_common(1)[0][0]\n",
    "    \n",
    "    return body_font_size\n",
    "\n",
    "def is_footer_or_header(text, page_height, y_position):\n",
    "    \"\"\"\n",
    "    Simple heuristic to filter out page numbers and copyright footers \n",
    "    based on vertical position (top 5% or bottom 10% of page).\n",
    "    \"\"\"\n",
    "    if y_position < (page_height * 0.05) or y_position > (page_height * 0.90):\n",
    "        # Specific filters for the document you provided\n",
    "        skip_keywords = [\"Page\", \"Copyright\", \"UnitedHealthcare\", \"Proprietary Information\", \"Effective\"]\n",
    "        if any(keyword in text for keyword in skip_keywords):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def list_to_markdown_table(table_data):\n",
    "    \"\"\"\n",
    "    Converts a list of lists (from pdfplumber) into a Markdown table.\n",
    "    \"\"\"\n",
    "    if not table_data:\n",
    "        return \"\"\n",
    "    \n",
    "    # Clean None values\n",
    "    clean_data = [[str(cell).replace('\\n', ' ').strip() if cell else \"\" for cell in row] for row in table_data]\n",
    "    \n",
    "    # Create header row\n",
    "    header = \"| \" + \" | \".join(clean_data[0]) + \" |\"\n",
    "    separator = \"| \" + \" | \".join([\"---\"] * len(clean_data[0])) + \" |\"\n",
    "    \n",
    "    body = \"\"\n",
    "    for row in clean_data[1:]:\n",
    "        body += \"\\n| \" + \" | \".join(row) + \" |\"\n",
    "        \n",
    "    return f\"\\n{header}\\n{separator}{body}\\n\"\n",
    "\n",
    "def convert_pdf_to_md(pdf_path, md_path):\n",
    "    print(f\"Processing {pdf_path}...\")\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        body_font_size = analyze_font_sizes(pdf)\n",
    "        print(f\"Detected Body Font Size: ~{body_font_size}pt\")\n",
    "        \n",
    "        markdown_content = []\n",
    "        \n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            print(f\"Scanning Page {i+1}...\")\n",
    "            \n",
    "            # 1. Extract Tables first to avoid duplicating text\n",
    "            tables = page.find_tables()\n",
    "            table_bboxes = [t.bbox for t in tables] # Get bounding boxes to exclude this text later\n",
    "            \n",
    "            extracted_tables = []\n",
    "            for table in page.extract_tables():\n",
    "                extracted_tables.append(list_to_markdown_table(table))\n",
    "            \n",
    "            # 2. Extract Text Line by Line\n",
    "            text_content = []\n",
    "            \n",
    "            # Extract words with layout info\n",
    "            words = page.extract_words(extra_attrs=[\"size\", \"top\", \"bottom\"])\n",
    "            \n",
    "            # Group words into lines based on \"top\" position (with small tolerance)\n",
    "            lines = {}\n",
    "            for w in words:\n",
    "                # Check if this word is inside a table; if so, skip it\n",
    "                in_table = False\n",
    "                w_center_x = (w['x0'] + w['x1']) / 2\n",
    "                w_center_y = (w['top'] + w['bottom']) / 2\n",
    "                \n",
    "                for bbox in table_bboxes:\n",
    "                    if (bbox[0] < w_center_x < bbox[2]) and (bbox[1] < w_center_y < bbox[3]):\n",
    "                        in_table = True\n",
    "                        break\n",
    "                if in_table:\n",
    "                    continue\n",
    "\n",
    "                # Group by Y position (rounded to combine words on same line)\n",
    "                y_pos = round(w['top'])\n",
    "                if y_pos not in lines:\n",
    "                    lines[y_pos] = []\n",
    "                lines[y_pos].append(w)\n",
    "            \n",
    "            # Sort lines by vertical position\n",
    "            sorted_y = sorted(lines.keys())\n",
    "            \n",
    "            for y in sorted_y:\n",
    "                line_words = lines[y]\n",
    "                # Reconstruct the line text\n",
    "                line_text = \" \".join([w['text'] for w in line_words])\n",
    "                \n",
    "                # Check if it's a footer/header garbage\n",
    "                if is_footer_or_header(line_text, page.height, y):\n",
    "                    continue\n",
    "                \n",
    "                # Determine Heading Level based on max font size in the line\n",
    "                max_size = max([w['size'] for w in line_words])\n",
    "                \n",
    "                # Heuristics for formatting\n",
    "                if max_size > body_font_size * 1.5:\n",
    "                    prefix = \"# \" # H1\n",
    "                elif max_size > body_font_size * 1.1:\n",
    "                    prefix = \"## \" # H2\n",
    "                else:\n",
    "                    prefix = \"\"\n",
    "                \n",
    "                # Bold logic (simplified: if the whole line is bold font)\n",
    "                # pdfplumber font names usually contain \"Bold\"\n",
    "                is_bold = all(\"Bold\" in w.get(\"fontname\", \"\") for w in line_words)\n",
    "                if is_bold and prefix == \"\":\n",
    "                    line_text = f\"**{line_text}**\"\n",
    "                \n",
    "                text_content.append(f\"{prefix}{line_text}\")\n",
    "\n",
    "            # 3. Assemble Page Content\n",
    "            # We append text, then insert tables at the end of the page (simplification)\n",
    "            # Or interleave them if we tracked Y-positions strictly. \n",
    "            # For this script, we append tables after text for cleaner reading if strict flow isn't critical.\n",
    "            \n",
    "            page_md = \"\\n\".join(text_content)\n",
    "            for tbl in extracted_tables:\n",
    "                # Heuristic: append tables at the end of the section\n",
    "                page_md += f\"\\n\\n{tbl}\\n\"\n",
    "                \n",
    "            markdown_content.append(page_md)\n",
    "\n",
    "    # Save to file\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        full_text = \"\\n\\n---\\n\\n\".join(markdown_content)\n",
    "        f.write(full_text)\n",
    "    \n",
    "    print(f\"Successfully saved to {md_path}\")\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual file name\n",
    "    input_pdf = \"spinal_pain.pdf\" \n",
    "    output_md = \"spinal_pain.md\"\n",
    "    \n",
    "    try:\n",
    "        convert_pdf_to_md(input_pdf, output_md)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {input_pdf}. Make sure the file is in the same folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e4ace",
   "metadata": {},
   "source": [
    "#Rag implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b32b3b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document \n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa318cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using lanchain's loaders\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\n",
    "    file_path=\"spinal_pain.md\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "md_text = documents[0].page_content\n",
    "\n",
    "# print(f\"Loaded {len(documents)} documents\")\n",
    "# print(type(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40dd1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"##\", \"H2\")],  # Targets file's ## headers\n",
    "    strip_headers=True\n",
    ")\n",
    "md_header_splits = splitter.split_text(md_text)\n",
    "\n",
    "# Only if split_text() returns dicts (older versions)\n",
    "chunks = []\n",
    "for split in md_header_splits:\n",
    "    if isinstance(split, dict):\n",
    "        chunks.append(Document(\n",
    "            page_content=split[\"content\"],\n",
    "            metadata={**documents[0].metadata, **split[\"metadata\"]}\n",
    "        ))\n",
    "    else:  # It's already a Document\n",
    "        split.metadata.update(documents[0].metadata)  # Merge source metadata\n",
    "        chunks.append(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250767d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First split type: <class 'langchain_core.documents.base.Document'>\n",
      "First split keys/attributes: ['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__copy__', '__deepcopy__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__fields__', '__fields_set__', '__firstlineno__', '__format__', '__ge__', '__get_pydantic_core_schema__', '__get_pydantic_json_schema__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__pretty__', '__private_attributes__', '__pydantic_complete__', '__pydantic_computed_fields__', '__pydantic_core_schema__', '__pydantic_custom_init__', '__pydantic_decorators__', '__pydantic_extra__', '__pydantic_fields__', '__pydantic_fields_set__', '__pydantic_generic_metadata__', '__pydantic_init_subclass__', '__pydantic_on_complete__', '__pydantic_parent_namespace__', '__pydantic_post_init__', '__pydantic_private__', '__pydantic_root_model__', '__pydantic_serializer__', '__pydantic_setattr_handlers__', '__pydantic_validator__', '__reduce__', '__reduce_ex__', '__replace__', '__repr__', '__repr_args__', '__repr_name__', '__repr_recursion__', '__repr_str__', '__rich_repr__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_get_value', '_iter', '_setattr_handler', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'id', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'metadata', 'model_computed_fields', 'model_config', 'model_construct', 'model_copy', 'model_dump', 'model_dump_json', 'model_extra', 'model_fields', 'model_fields_set', 'model_json_schema', 'model_parametrized_name', 'model_post_init', 'model_rebuild', 'model_validate', 'model_validate_json', 'model_validate_strings', 'page_content', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'type', 'update_forward_refs', 'validate']\n",
      "First split: page_content='# Ablative Treatment for Spinal Pain\n",
      "Policy Number: 2026T0107II\n",
      "Effective Date: February 1, 2026  Instructions for Use\n",
      "Table of Contents Page\n",
      "Application .......................................................................... 1\n",
      "Coverage Rationale ............................................................ 1\n",
      "Definitions ........................................................................... 2\n",
      "Applicable Codes ................................................................ 2\n",
      "Description of Services........................................................ 2\n",
      "Clinical Evidence ................................................................. 3\n",
      "U.S. Food and Drug Administration ................................... 11\n",
      "References ....................................................................... 11\n",
      "Policy History/Revision Information ................................... 14\n",
      "Instructions for Use ........................................................... 14'\n"
     ]
    }
   ],
   "source": [
    "# md_header_splits = splitter.split_text(md_text)\n",
    "# print(\"First split type:\", type(md_header_splits[0]))\n",
    "# print(\"First split keys/attributes:\", dir(md_header_splits[0]))\n",
    "# print(\"First split:\", md_header_splits[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f43c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "    \n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1a2305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreiver = vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3})\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-3-flash-preview\",\n",
    "#     temperature=0.3,\n",
    "#     max_output_tokens=512,\n",
    "#     google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "# )\n",
    "llm = ChatOpenAI(\n",
    "    model=\"arcee-ai/trinity-large-preview:free\",  # example\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87555a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "This is for your reference: <<CONTEXT>>\n",
    "\n",
    "You are a healthcare payer policy compiler.\n",
    "\n",
    "You are NOT a summarizer.\n",
    "You are NOT a clinical interpreter.\n",
    "You are NOT allowed to infer coverage.\n",
    "You are NOT allowed to create logic from clinical trials.\n",
    "\n",
    "Your job is to extract enforceable payer coverage rules EXACTLY as written.\n",
    "\n",
    "You are compiling machine-executable coverage logic.\n",
    "\n",
    "------------------------------------------------------------\n",
    "EXTRACTION PRIORITY (STRICT ORDER OF AUTHORITY)\n",
    "------------------------------------------------------------\n",
    "\n",
    "1) Coverage Rationale section (highest authority)\n",
    "2) Explicit coverage / non-coverage statements\n",
    "3) Conditional coverage language (\"covered when\", \"medically necessary when\")\n",
    "4) CPT / HCPCS applicability sections\n",
    "5) Frequency limitations\n",
    "6) Site-of-service requirements\n",
    "7) Modifier requirements\n",
    "8) Cross-policy dependencies\n",
    "9) Clinical Evidence (for citation only; NEVER for eligibility logic)\n",
    "\n",
    "------------------------------------------------------------\n",
    "HARD CLASSIFICATION RULES\n",
    "------------------------------------------------------------\n",
    "\n",
    "If the policy uses ANY of the following phrases:\n",
    "\n",
    "- \"unproven\"\n",
    "- \"not medically necessary\"\n",
    "- \"investigational\"\n",
    "- \"experimental\"\n",
    "- \"not covered\"\n",
    "- \"insufficient evidence\"\n",
    "\n",
    "THEN:\n",
    "\n",
    "coverage_status = \"not_covered\"\n",
    "enforcement_level = \"hard\"\n",
    "decision_type = \"investigational_or_exclusion\"\n",
    "auto_denial = true\n",
    "\n",
    "NO CONDITIONS should be extracted for these procedures.\n",
    "\n",
    "------------------------------------------------------------\n",
    "CONDITIONAL COVERAGE EXTRACTION RULES\n",
    "------------------------------------------------------------\n",
    "\n",
    "Only extract conditions IF AND ONLY IF the policy explicitly states:\n",
    "\n",
    "- \"covered when\"\n",
    "- \"medically necessary when\"\n",
    "- \"considered medically necessary when\"\n",
    "- \"coverage is provided when\"\n",
    "\n",
    "If such language does not exist:\n",
    "→ Do NOT invent conditions.\n",
    "→ Do NOT infer requirements.\n",
    "→ Mark as hard denial if exclusionary language exists.\n",
    "\n",
    "------------------------------------------------------------\n",
    "CRITICAL EXTRACTION REQUIREMENTS\n",
    "------------------------------------------------------------\n",
    "\n",
    "For EACH distinct procedure or technique:\n",
    "\n",
    "1) Create a SEPARATE rule object.\n",
    "   - Do NOT merge multiple techniques into one.\n",
    "   - Each technique = one atomic rule.\n",
    "\n",
    "2) Extract ALL CPT and HCPCS codes if listed.\n",
    "   - If codes exist in a separate section, attach them.\n",
    "   - If no codes are listed, return empty array [].\n",
    "   - Never hallucinate codes.\n",
    "\n",
    "3) Preserve EXACT policy denial language in:\n",
    "   \"explicit_policy_language\"\n",
    "\n",
    "4) Extract frequency limits ONLY if explicitly stated.\n",
    "\n",
    "5) Extract site-of-service requirements ONLY if explicitly stated.\n",
    "\n",
    "6) Extract modifier requirements ONLY if explicitly stated.\n",
    "\n",
    "7) If policy references another policy:\n",
    "   - Include reference name in \"referenced_policies\"\n",
    "   - Do NOT extract logic from referenced document unless present in context.\n",
    "\n",
    "------------------------------------------------------------\n",
    "CLINICAL EVIDENCE SECTION HANDLING\n",
    "------------------------------------------------------------\n",
    "\n",
    "Clinical Evidence sections:\n",
    "- Explain rationale\n",
    "- Do NOT define eligibility\n",
    "- Do NOT convert study criteria into coverage rules\n",
    "- May be used ONLY for citation context\n",
    "\n",
    "------------------------------------------------------------\n",
    "CONFLICT RESOLUTION\n",
    "------------------------------------------------------------\n",
    "\n",
    "If Clinical Evidence contradicts Coverage Rationale:\n",
    "Coverage Rationale ALWAYS overrides.\n",
    "\n",
    "------------------------------------------------------------\n",
    "OUTPUT REQUIREMENTS\n",
    "------------------------------------------------------------\n",
    "\n",
    "Return STRICT VALID JSON only.\n",
    "No markdown.\n",
    "No explanation.\n",
    "No commentary.\n",
    "No summaries.\n",
    "\n",
    "Schema:\n",
    "\n",
    "{\n",
    "  \"metadata\": {\n",
    "    \"payer\": \"\",\n",
    "    \"policy_name\": \"\",\n",
    "    \"policy_number\": \"\",\n",
    "    \"effective_date\": \"\",\n",
    "    \"plan_type\": \"\",\n",
    "    \"region\": \"\"\n",
    "  },\n",
    "  \"procedures\": [\n",
    "    {\n",
    "      \"procedure_name\": \"\",\n",
    "      \"codes\": [],\n",
    "      \"coverage_status\": \"covered | conditional | not_covered\",\n",
    "      \"decision_type\": \"\",\n",
    "      \"enforcement_level\": \"hard | conditional\",\n",
    "      \"auto_denial\": true | false,\n",
    "      \"explicit_policy_language\": \"\",\n",
    "      \"conditions\": [],\n",
    "      \"frequency_limits\": [],\n",
    "      \"site_of_service_requirements\": [],\n",
    "      \"modifier_requirements\": [],\n",
    "      \"referenced_policies\": []\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Never summarize.\n",
    "Never infer.\n",
    "Never soften denial language.\n",
    "Never generate hypothetical eligibility.\n",
    "\n",
    "Return valid JSON only.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b63148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str):\n",
    "    docs = retreiver.invoke(question)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.replace(\"<<CONTEXT>>\", context)\n",
    "    # system_prompt = SYSTEM_PROMPT_TEMPLATE.format(context=context)\n",
    "    response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=question)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON file saved as: features1.json\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_json(data, filename=None):\n",
    "    # Auto-generate filename if not provided\n",
    "    if filename is None:\n",
    "        filename = \"features1.json\"  # Changed extension\n",
    "    \n",
    "    # Handle both dict/list input and JSON string input\n",
    "    if isinstance(data, (dict, list)):\n",
    "        json_str = json.dumps(data, indent=2, ensure_ascii=False)\n",
    "    else:\n",
    "        # Assume it's already JSON string (from your LLM output)\n",
    "        json_str = data.strip(\"```json\\n\").strip(\"```\").strip()\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json_str)\n",
    "    \n",
    "    print(f\"JSON file saved as: {filename}\")\n",
    "    \n",
    "    \n",
    "a = answer_question(\"Analyze the spinal-pain policy and give insights on procedures that must be preffered in order to treat a patient and satisfies the policy rules.\")\n",
    "save_json(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
